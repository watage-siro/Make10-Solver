name: Benchmark Make10 (all metrics)

on:
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Generate inputs
        run: |
          python3 bench/input.py

      - name: Build
        run: |
          g++ -O2 -std=c++17 -o make10 main.cpp

      - name: Run benchmark on generated inputs
        run: |
          mkdir -p bench/results
          echo "input,run1_calls,run1_ops,run1_evals,run1_time,run2_calls,run2_ops,run2_evals,run2_time,run3_calls,run3_ops,run3_evals,run3_time,run4_calls,run4_ops,run4_evals,run4_time,run5_calls,run5_ops,run5_evals,run5_time,avg_calls,avg_ops,avg_evals,avg_time,min_calls,max_calls,min_time,max_time" > bench/results/results.csv

          for input in bench/inputs/*.txt; do
            results=()
            for i in $(seq 1 5); do
              out=$(./make10 < "$input")
              results+=($out)
            done

            line=$(python3 - "${results[@]}" <<'PY'
import statistics, sys
data = sys.argv[1:]
calls = [int(x) for x in data[0::4]]
ops   = [int(x) for x in data[1::4]]
evals = [int(x) for x in data[2::4]]
times = [float(x) for x in data[3::4]]

output = []
for i in range(5):
    output.extend([calls[i], ops[i], evals[i], times[i]])

output.extend([
    statistics.mean(calls), statistics.mean(ops), statistics.mean(evals), statistics.mean(times),
    min(calls), max(calls), min(times), max(times)
])
print(",".join(map(str, output)))
PY
)
            echo "$(basename "$input"),$line" >> bench/results/results.csv
          done

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench/results/results.csv
